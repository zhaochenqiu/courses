{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYz-EHRRjR5s"
   },
   "source": [
    "# Multivariate function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "iBdw1ckMUomp",
    "outputId": "1e18e61b-4748-4f11-cfd1-2b6a4c390851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector variable x: tensor([ 3., -1.,  0.,  1.])\n",
      "Function f at x: tensor(215.)\n",
      "Gradient of f at x: tensor([ 306., -144.,   -2., -310.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
    "f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
    "\n",
    "print(\"Vector variable x:\",x.data)\n",
    "\n",
    "print(\"Function f at x:\",f.data)\n",
    "\n",
    "# compute gradient of f at x\n",
    "g = torch.autograd.grad(f,x)\n",
    "\n",
    "print(\"Gradient of f at x:\",g[0].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KTUnFP6tjemZ"
   },
   "source": [
    "#Hessian computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ChfNnPqrVTII",
    "outputId": "954ad2d9-428c-4ce6-9bd4-edac62529cfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian: tensor([[ 482.,   20.,    0., -480.],\n",
      "        [  20.,  212.,  -24.,    0.],\n",
      "        [   0.,  -24.,   58.,  -10.],\n",
      "        [-480.,    0.,  -10.,  490.]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
    "f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
    "\n",
    "# PyTorch does not compute Hessian (second order derivatives) directly\n",
    "# PyTorch can compute Jacobian vector product \n",
    "# We can use Jacobian vector product to compute Hessian\n",
    "\n",
    "# Step 1: compute gradient\n",
    "g = torch.autograd.grad(f,x,retain_graph=True,create_graph=True) # compute gradient with two important flags on\n",
    "\n",
    "# Step 2: Use product of Jacobian of g and columns of identity matrix to compute hessian of f\n",
    "eye = torch.eye(4) # 4-by-4 identity matrix\n",
    "H = torch.stack([torch.autograd.grad(g,x,eye[:,i],retain_graph=True)[0] for i in range(4)]) # hessian\n",
    "\n",
    "print(\"Hessian:\",H.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KB85808hjuOK"
   },
   "source": [
    "#Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "PYaX2d0PYmOu",
    "outputId": "c401f8ab-8229-47c9-c4ee-2b0fa90a08ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current variable value: [ 2.694e+00 -8.560e-01  2.000e-03  1.310e+00] Current function value: 215.0\n",
      "Current variable value: [ 1.7120734  -0.15217544  0.43262678  1.239527  ] Current function value: 4.898355960845947\n",
      "Current variable value: [ 1.3354341  -0.12037495  0.38850418  0.923193  ] Current function value: 2.40033221244812\n",
      "Current variable value: [ 1.0786515  -0.0987131   0.3482537   0.71352595] Current function value: 1.2607851028442383\n",
      "Current variable value: [ 0.89774257 -0.08322652  0.31416288  0.57114387] Current function value: 0.708449125289917\n",
      "Current variable value: [ 0.7665225  -0.07183251  0.28545532  0.47160053] Current function value: 0.42393937706947327\n",
      "Current variable value: [ 0.6686587  -0.06322152  0.26128545  0.39994755] Current function value: 0.2685073912143707\n",
      "Current variable value: [ 0.593754   -0.0565507   0.2408716   0.34689713] Current function value: 0.17878548800945282\n",
      "Current variable value: [ 0.53504807 -0.05126574  0.2235377   0.30656764] Current function value: 0.12432404607534409\n",
      "Current variable value: [ 0.48804614 -0.04699376  0.208721    0.27515563] Current function value: 0.08974049240350723\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
    "  \n",
    "steplength = 1e-3 # for gradient descent\n",
    "for i in range(1000):\n",
    "  # function\n",
    "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
    "  # compute grdaient\n",
    "  g = torch.autograd.grad(f,x)\n",
    "  # adjust variable\n",
    "  x = x - steplength*g[0]\n",
    "  if i%100==0:\n",
    "    print(\"Current variable value:\",x.detach().numpy(),\"Current function value:\", f.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itsqyBm0j22X"
   },
   "source": [
    "#Gradient descent using PyTorch's optmization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "HfW8cdRDiZYz",
    "outputId": "c2458b2f-4896-4e35-9999-fe9484eaf454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current variable value: [ 2.6940000e+00 -8.5600001e-01  2.0000001e-03  1.3100001e+00] Current function value: 215.0\n",
      "Current variable value: [ 1.5316538  -0.21989448  0.73716354  1.6671876 ] Current function value: 13.013964653015137\n",
      "Current variable value: [ 1.291658   -0.01572123  0.0497352   0.90083665] Current function value: 6.999632835388184\n",
      "Current variable value: [0.4704115  0.03482796 0.07995438 0.83619183] Current function value: 3.9561965465545654\n",
      "Current variable value: [ 0.24767333 -0.05528516  0.43611085  0.3287799 ] Current function value: 1.0580174922943115\n",
      "Current variable value: [ 0.16845866 -0.04593072  0.25198996  0.17555024] Current function value: 0.18354195356369019\n",
      "Current variable value: [0.14139268 0.00461269 0.06891006 0.15731733] Current function value: 0.10066982358694077\n",
      "Current variable value: [ 0.1291523  -0.0037347   0.04939296  0.10238892] Current function value: 0.019815834239125252\n",
      "Current variable value: [ 0.12336344 -0.01970605  0.07073621  0.05433257] Current function value: 0.010082602500915527\n",
      "Current variable value: [ 0.12145541 -0.01454422  0.06758722  0.04719672] Current function value: 0.0033138450235128403\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
    "  \n",
    "optimizer = optim.SGD([x], lr=1e-3, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
    "\n",
    "for i in range(100):\n",
    "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
    "  optimizer.zero_grad()\n",
    "  f.backward()\n",
    "  optimizer.step()\n",
    "  if i%10==0:\n",
    "    print(\"Current variable value:\",x.detach().numpy(),\"Current function value:\", f.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDghJQA9kf6L"
   },
   "source": [
    "#Newton's method (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "ynGB444bke1c",
    "outputId": "de509019-83cc-48a1-8ddc-8e374d0d3858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5873, -0.1587,  0.2540,  0.2540])\n",
      "tensor(31.8025)\n",
      "tensor([ 1.0582, -0.1058,  0.1693,  0.1693])\n",
      "tensor(6.2820)\n",
      "tensor([ 0.7055, -0.0705,  0.1129,  0.1129])\n",
      "tensor(1.2409)\n",
      "tensor([ 0.4703, -0.0470,  0.0752,  0.0752])\n",
      "tensor(0.2451)\n",
      "tensor([ 0.3135, -0.0314,  0.0502,  0.0502])\n",
      "tensor(0.0484)\n",
      "tensor([ 0.2090, -0.0209,  0.0334,  0.0334])\n",
      "tensor(0.0096)\n",
      "tensor([ 0.1394, -0.0139,  0.0223,  0.0223])\n",
      "tensor(0.0019)\n",
      "tensor([ 0.0929, -0.0093,  0.0149,  0.0149])\n",
      "tensor(0.0004)\n",
      "tensor([ 0.0619, -0.0062,  0.0099,  0.0099])\n",
      "tensor(7.3712e-05)\n",
      "tensor([ 0.0413, -0.0041,  0.0066,  0.0066])\n",
      "tensor(1.4560e-05)\n"
     ]
    }
   ],
   "source": [
    "# Newton's optimization for an example - Powell Function (https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf)\n",
    "# Minimize Powell function: f(x1,x2,x3,x4) = (x1+10x2)^2 + 5(x3-x4)^2 + (x2-2x3)^4 + 10(x1-x4)^4\n",
    "\n",
    "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
    "f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
    "\n",
    "def LineSearch(x,d):\n",
    "  minstep = 1.0\n",
    "  minval=1e10\n",
    "  for i in range(10):\n",
    "    step = (i+1)/10.0\n",
    "    xp = x.data + step*d.data\n",
    "    fval = (xp[0]+10.0*xp[1])**2 + 5.0*(xp[2]-xp[3])**2 + (xp[1]-2.0*xp[2])**4 + 10.0*(xp[0]-xp[3])**4\n",
    "    if fval < minval:\n",
    "      minval = fval\n",
    "      minstep = step\n",
    "  return minstep\n",
    "\n",
    "eye = torch.eye(4)\n",
    "\n",
    "for itr in range(10):\n",
    "  # Step 1: compute Newton direction d\n",
    "  g = torch.autograd.grad(f,x,retain_graph=True,create_graph=True) # gradient\n",
    "  H = torch.stack([torch.autograd.grad(g,x,eye[:,i],retain_graph=True)[0] for i in range(4)]) # hessian\n",
    "  d = torch.solve(-g[0].unsqueeze(1), H)[0].t().squeeze() # solve Newton system\n",
    "  \n",
    "  # Step 2: update x with Newton direction d\n",
    "  step_length = LineSearch(x,d)\n",
    "  x.data += step_length*d.data # often step_length is set as 1.0\n",
    "  print(x.data)\n",
    "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
    "  print(f.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9qccXCcknY8"
   },
   "source": [
    "#Conjugate gradient method (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "KLI5FGLeQxUO",
    "outputId": "4b7fadf2-98cf-427c-f5de-1a2001fe8aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5873, -0.1587,  0.2540,  0.2540])\n",
      "tensor(31.8025)\n",
      "tensor([ 1.0582, -0.1058,  0.1693,  0.1693])\n",
      "tensor(6.2820)\n",
      "tensor([ 0.7055, -0.0705,  0.1129,  0.1129])\n",
      "tensor(1.2409)\n",
      "tensor([ 0.4703, -0.0470,  0.0752,  0.0752])\n",
      "tensor(0.2451)\n",
      "tensor([ 0.3135, -0.0314,  0.0502,  0.0502])\n",
      "tensor(0.0484)\n",
      "tensor([ 0.2090, -0.0209,  0.0334,  0.0334])\n",
      "tensor(0.0096)\n",
      "tensor([ 0.1394, -0.0139,  0.0223,  0.0223])\n",
      "tensor(0.0019)\n",
      "tensor([ 0.0929, -0.0093,  0.0149,  0.0149])\n",
      "tensor(0.0004)\n",
      "tensor([ 0.0619, -0.0062,  0.0099,  0.0099])\n",
      "tensor(7.3712e-05)\n",
      "tensor([ 0.0413, -0.0041,  0.0066,  0.0066])\n",
      "tensor(1.4561e-05)\n"
     ]
    }
   ],
   "source": [
    "# Newton method using conjugate gradient (https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n",
    "# Hessian-vector product computation needs one autograd call per iteration inside CG iteration\n",
    "# So this method never computes and stores the full hessian matrix\n",
    "# CG solver might converge faster than other general linear equation solver\n",
    "# Also, it seems to be more stable\n",
    "\n",
    "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
    "f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
    "\n",
    "for itr in range(10):\n",
    "  # Step 1: compute Newton direction d by CG method\n",
    "  g = torch.autograd.grad(f,x,retain_graph=True,create_graph=True) # gradient\n",
    "  r = -g[0].data.clone()\n",
    "  p = r.clone()\n",
    "  d = torch.tensor([0.,0.,0.,0.])\n",
    "  rsold = torch.sum(r**2)\n",
    "  for cg_itr in range(6): # cg_itr should be slightly larger length of variable - here variable length is 4\n",
    "    q = torch.autograd.grad(g,x,p,retain_graph=True)[0] # hessian-vector (Jacobian-vector) product computation by autograd\n",
    "    alpha = rsold/torch.sum(p*q)\n",
    "    d += alpha*p\n",
    "    r += -alpha*q\n",
    "    rsnew = torch.sum(r**2)\n",
    "    if rsnew<1e-10:\n",
    "      break\n",
    "    p = r + (rsnew/rsold)*p\n",
    "    rsold = rsnew\n",
    "    \n",
    "  # Step 2: update x with Newton direction d\n",
    "  step_length = LineSearch(x,d)\n",
    "  x.data += step_length*d.data # often step_length is set as 1.0\n",
    "  print(x.data)\n",
    "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
    "  print(f.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9OZiFkLnNhT"
   },
   "source": [
    "#Chain rule of derivative and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5tNYtd96zEPH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "80oOHqfynMjr",
    "outputId": "b5825393-0850-49d2-e8d7-5711d9ce5d9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable z: tensor([ 1., -1.], requires_grad=True)\n",
      "function x: tensor([2., 1., 1., 0.], grad_fn=<CopySlices>)\n",
      "function f: tensor(310., grad_fn=<AddBackward0>)\n",
      "\n",
      "Gradient by chain rule: tensor([ 486., -710.])\n",
      "Gradient by PyTorch: tensor([ 486., -710.])\n"
     ]
    }
   ],
   "source": [
    "z = Variable(torch.tensor([1.0,-1.0]),requires_grad=True)\n",
    "\n",
    "print(\"Variable z:\",z)\n",
    "\n",
    "def compute_x(z):\n",
    "  x = torch.zeros(4)\n",
    "  x[0] = z[0] - z[1]\n",
    "  x[1] = z[0]**2\n",
    "  x[2] = z[1]**2\n",
    "  x[3] = z[0]**2+z[0]*z[1]\n",
    "  return x\n",
    "\n",
    "x = compute_x(z)\n",
    "print(\"function x:\",x)\n",
    "\n",
    "def compute_f(x):\n",
    "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
    "  return f\n",
    "\n",
    "f = compute_f(x)\n",
    "print(\"function f:\",f)\n",
    "print(\"\")\n",
    "# Cahin \n",
    "# Let's compute gradient of f with respect to x\n",
    "g_x = torch.autograd.grad(f,x,retain_graph=True,create_graph=True)\n",
    "# Now compute Jacobian of x with respect to z and multiply with g_x to use chain rule\n",
    "g_z = torch.autograd.grad(x,z,g_x,retain_graph=True) \n",
    "\n",
    "# But PyTorch can compute derivative of f with respect to z directly - this is the amazing capability!\n",
    "g = torch.autograd.grad(f,z)\n",
    "\n",
    "print(\"Gradient by chain rule:\",g_z[0])\n",
    "print(\"Gradient by PyTorch:\",g[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RyEZ0Qy_zIFN"
   },
   "source": [
    "#Optimization with respect to z by gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "igcWXotzzWfz",
    "outputId": "3ed5adaa-2b38-4247-bd54-217926f3beec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current variable value: [-0.02480921 -0.02491281] Current function value: 4.123583494219929e-05\n",
      "Current variable value: [-0.02531579 -0.02381584] Current function value: 2.6499987143324688e-05\n",
      "Current variable value: [-0.02569171 -0.0229568 ] Current function value: 1.7627042325329967e-05\n",
      "Current variable value: [-0.02596967 -0.02228188] Current function value: 1.2251370208105072e-05\n",
      "Current variable value: [-0.02617342 -0.02174992] Current function value: 8.977720426628366e-06\n",
      "Current variable value: [-0.02632053 -0.02132925] Current function value: 6.974322332098382e-06\n",
      "Current variable value: [-0.02642414 -0.02099535] Current function value: 5.741535915149143e-06\n",
      "Current variable value: [-0.02649423 -0.02072919] Current function value: 4.977613116352586e-06\n",
      "Current variable value: [-0.02653841 -0.02051599] Current function value: 4.499648184719263e-06\n",
      "Current variable value: [-0.02656251 -0.02034421] Current function value: 4.196362624497851e-06\n"
     ]
    }
   ],
   "source": [
    "steplength = 1e-3 # for gradient descent\n",
    "for i in range(1000):\n",
    "  # function\n",
    "  f = compute_f(compute_x(z))\n",
    "  # Compute gradient of f with respect to z directly\n",
    "  # PyTorch takes care of chain rule of derivatives\n",
    "  g = torch.autograd.grad(f,z) \n",
    "  # adjust variable\n",
    "  z = z - steplength*g[0]\n",
    "  if i%100==0:\n",
    "    print(\"Current variable value:\",z.detach().numpy(),\"Current function value:\", f.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1MbO8BG0Gzk"
   },
   "source": [
    "#And of course optimization using PyTorch's gradient descent optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "bVsIisPK0RCk",
    "outputId": "824fb3ac-f977-4ff1-a2b2-f8db467e3c1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current variable value: [ 0.514      -0.28999996] Current function value: 310.0\n",
      "Current variable value: [ 0.29414672 -0.63746554] Current function value: 78.43021392822266\n",
      "Current variable value: [0.303851   0.03234617] Current function value: 18.59418487548828\n",
      "Current variable value: [0.24348229 0.30544168] Current function value: 0.07254856824874878\n",
      "Current variable value: [-0.0413984   0.35884088] Current function value: 0.3099023699760437\n",
      "Current variable value: [-0.22963157  0.17217131] Current function value: 0.4768257737159729\n",
      "Current variable value: [-0.05684688 -0.02945037] Current function value: 0.0002854902413673699\n",
      "Current variable value: [ 0.00743173 -0.10030661] Current function value: 0.011820731684565544\n",
      "Current variable value: [ 0.0100228  -0.10663033] Current function value: 0.017412500455975533\n",
      "Current variable value: [-0.01059541 -0.08887993] Current function value: 0.007626336999237537\n"
     ]
    }
   ],
   "source": [
    "z = Variable(torch.tensor([1.0,-1.0]),requires_grad=True)\n",
    "  \n",
    "optimizer = optim.SGD([z], lr=1e-3, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
    "\n",
    "for i in range(100):\n",
    "  f = compute_f(compute_x(z))\n",
    "  optimizer.zero_grad()\n",
    "  f.backward()\n",
    "  optimizer.step()\n",
    "  if i%10==0:\n",
    "    print(\"Current variable value:\",z.detach().numpy(),\"Current function value:\", f.item())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Partial_Derivatives_and_GD_and_chain_rule.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
